\documentclass[a4paper,12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{xcolor}
\usepackage{ulem}        % underline
\usepackage{tcolorbox}   % colored boxes (if needed)
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{centernot}
\usetikzlibrary{trees} % for tree structures

\begin{document}

\begin{center}
\begin{center}
    \huge{\textbf{Math√©matiques : CM}}
\end{center}
\end{center}
\newpage

\tableofcontents 


\newpage



\section{Euclidean space - scalar product}

\textbf{Idea : Let E a vector space of finite dimension} \bigskip

The scalar product is defined as :

\[u \in E, v \in E, <u, v>\]

\[< , > \; = \; \begin{cases} E \times E \; \rightarrow \; \mathbb{R} \\
    (u, v) \; \mapsto \; <u, v> 
    \end{cases} 
\] \bigskip

When u and v are orthogonal ($\perp$) we have the following relation :

\[ u \perp v \equiv <u, v> = 0
\]

We also have the following relation : \\

\[
||u|| = \sqrt{<u, u>}
\] \\

\textbf{Standard scalar product of $\mathbb{R}^3$ :} \\

\[
u = (x_1, y_1, z_1) \\
v = (x_2, y_2, z_2)
\]

\[<u,v> = x_1x_2 + y_1y_2 + z_1z_2\]
\[\implies ||u|| = \sqrt{<u,u>} = \sqrt{x_1^2 + y_1^2 + z_1^2}\] \\

\textbf{Definition : symmetric bilinear form} \\

Let a mapping such that :

\[
\phi : \begin{cases}
    E \times E \; \rightarrow \; \mathbb{R} \\
    (u, v)\; \mapsto \phi(u,v)
\end{cases}
\]

$\phi$ is a symmetric bilinear form if :

\begin{enumerate}
    \item It is bilinear : $\forall \; u_0 \in \, E, v \, \mapsto \, \phi (u_0, v)$ is a linear map
    \item It is bilinear : $\forall \; v_0 \in \, E, u \, \mapsto \, \phi (u, v_0)$ is a linear map
    \item It is symmetric : $\forall \; (u,v) \in \, E^2, \phi (u, v) = \phi(v,u)$ is a linear map
\end{enumerate} \bigskip

\textbf{Explanation :}

\[
\phi = \begin{cases}
    E \times E \; \rightarrow \, \mathbb{R} \\
    (u, v) \; \mapsto \phi (u,v)
\end{cases}
\]

Let $u_0 \in E$ : \\

The mapping : $\begin{cases}
    E \, \rightarrow \, \mathbb{R} \\
    v \, \mapsto \phi(u_0, v)
\end{cases}$is linear \\

\[
\forall \, (v, v') \in E^2, \forall \alpha \in \mathbb{R}, \phi(u_0, \alpha v + v') = \alpha \phi(u_0,v) + \phi(u_0, v')
\] \\

If $v_0 \in E, \forall (u,u') \in E^2,\forall \alpha \in \mathbb{R}, \phi(\alpha u + u', v_0) = \alpha \phi(u,v_0) + \phi(u', v_0) $ \\

\textbf{Example : standard scalar product of $\mathbb{R}^3$}

\[
u = (x_1, y_1, z_1) \\
v = (x_2, y_2, z_2)
\]

\[
\phi(u,v) = x_1x_2+ y_1y_2 + z_1z_2
\]

If u is given (considered as a constant), then the mapping  : \\

$v \, \mapsto \, x_1x_2 + y_1y_2 + z_1z_2$ is a linear map. \\

Note that the coordinates of u are constant numbers. \\ 

Those of v are variables.\\

for example if $u = (1,2,3)$, then $v = (x_2, 2y_2, 3z_2)$ \\

\textbf{Similarly :}, if $v = (x_2, y_2, z_2)$ is given, then the mapping : \\

$u \, \mapsto \, x_1x_2 + y_1y_2 + z_1z_2$ is a linear map . \\

Note that the coordinates of v are being constant numbers. \\

Those of u are variables. \\

\textbf{Furthermore}, $\phi(u,v) = \phi(v,u)$ \\

\textbf{A symmetric bilinear form $\phi$ is positive definite if :}

\[
\forall u \in E, \phi(u,u) \geq 0 \land \phi(u,u) = 0 \implies u = 0_E
\]

Example 1 : Standard scalar product of $\mathbb{R}^3$

\[
\phi(u,u) = x_1^2 + y_1^2 + z_1^2 \geq 0
\]

and \[
\phi(u,u) = 0 \implies \begin{cases}
    x_1^2 = 0 \\
    y_1^2 = 0 \\
    z_1^2 = 0 
\end{cases} \implies u = 0_E
\]

The standard scalar product is thus positive definite \\

Example 2 :

\[
\phi (u,v) = x_1x_2 + y_1y_2 - z_1z_2 \text{ is a symmetric bilinear form}
\] \bigskip

If $u = (x_1, y_1, z_1)$ is given (considered as a constant), then the mapping : \\

$v \, \mapsto \, x_1x_2 + y_1y_2 + z_1z_2$ is a linear map.\\

Note that the coordinates of u are constant numbers. \\

Those of v are the variables. \\

Thus the coordinates of u are the coefficient of the linear function. \\

Similarly, if v is given and constant, $u \mapsto \phi(u,v)$ is linear \\

\[\implies \phi(u,v) = \phi(v,u)\] \\

Positive definite ? No \\

\[
\phi(u,u) x_1^2 + y_1^2 - z_1^2
\] \bigskip

If $u = (1,0,0), \phi(u,u) = 1 > 0$ \\

If $u = (0,0,1), \phi(u,u) = -1 < 0$ \\

Example 3 :

\[
\phi (u,v) = x_1x_2 + y_1y_2
\phi(u,u) = x_1^2 + y_1^2 \geq 0
\]\bigskip

But $\phi(u,u) = 0 \centernot\implies u = 0_E$ as for $u=(0,0,1), \phi(u,u) = 0, u \neq 0_E$ \\

A scalar product is a symmetric bilinear form, which is positive definite. \\

Then we note $<u,v>$ instead of $\phi(u,v)$ \\

We can define for $u\in E$, 

\[
||u|| = \sqrt{<u,v>} \implies \sqrt{<u,v>} \in \mathbb{R} \text{ because } <u,v> \; \geq 0
\]

\[
||u|| = 0 \implies u = 0_E
\] \bigskip

Theorem : Matrix representation of a symmetric bilinear form :\\

Let $\mathcal{B} = (e_1, ..., e_n)$ be a basis of E. \\

u has coordinates : $\begin{pmatrix}
    x_1 \\
    y_1 \\
    z_1 \\
    ... \\
\end{pmatrix}$ in $\mathcal{B}$ \bigskip


v has coordinates : $\begin{pmatrix}
    x_2 \\
    y_2 \\
    z_2 \\
    ... \\
\end{pmatrix}$ in $\mathcal{B}$ \bigskip

Then : $\exists \, A \in \mathbb{M}_n(\mathbb{R}), \phi(u,v) = \,^+{X}_1AX_2$ is symmetric \\

Examples : \\

Standard scalar product of $\mathbb{R}^3$ : \\

Example 1 : \\

$\mathcal{B}$ standard basis of $\mathbb{R}^3$ \\

$u = (x_1, y_1, z_1)$ has coordinates $X_1 = \begin{pmatrix}
    x_1 \\
    y_1 \\
    z_1 \\
\end{pmatrix}$ \bigskip

$v = (x_2, y_2, z_2)$ has coordinates $X_2 = \begin{pmatrix}
    x_2 \\
    y_2 \\
    z_2 \\
\end{pmatrix}$ \bigskip

\[
^T{X_1}X_2 = (x_1, y_1, y_2) \begin{pmatrix}
    x_2\\
    y_2\\
    z_2\\
\end{pmatrix}
\]\bigskip

\[
= x_1x_2 + y_1y_2 +z_1z_2 \implies A = I_3 = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
\end{pmatrix}
\]

\[
^T{X_1}AX_2 = x_1x_2 + y_1y_2 + z_1z_2 = <u,v>
\] \bigskip

Example 2 :

\[
\phi(u,v) = x_1x_2 + y_1y_2 - z_1z_2
\]

\[\text{Then : } A = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & -1
\end{pmatrix}\] \bigskip

\[^T{X_1}AX_2 = (x_1, y_1, z_1) \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & -1
\end{pmatrix} \begin{pmatrix}
    x_2 \\
    y_2 \\
    z_2
\end{pmatrix}\]\bigskip

\[
= (x_1, y_1, z_1) \begin{pmatrix}
    x_2 \\
    y_2 \\
    -z_2
\end{pmatrix}
\] 

\[
= x_1x_2 + y_1y_2 - z_1z_2
\]

Example 3 : 

\[
\phi(u,v) = x_1x_2 + y_1y_2
\]

\[ \text{Then : } A = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 0
\end{pmatrix}\] \bigskip

\[^T{X_1}AX_2 = (x_1, y_1, z_1) \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 0
\end{pmatrix} \begin{pmatrix}
    x_2 \\
    y_2 \\
    z_2
\end{pmatrix}\] \bigskip

\[
= (x_1, y_1, z_1) \begin{pmatrix}
    x_2 \\
    y_2 \\
    0
\end{pmatrix}
\]

\[
= x_1x_2 + y_1y_2
\]
\bigskip

\subsection{Spectral Theorem} 
\bigskip

\textbf{Spectral Theorem :} all symmetric matrix A is diagonalizable. \\

We can choose an orthonormal eigenbasis ($\epsilon_1, ..., \epsilon_n$). \\

Then the transition matrix P satisfies to :

\[
^TPP = P \times {^T}P = I \Leftrightarrow P^{-1} = {^T}P
\] \\

Note : if the eigenbasis is orthonormal, then : 

\[
<\epsilon_i, \epsilon_j> = \begin{cases}
    1 \text{ if }i = j \\
    0 \text{ otherwise}
\end{cases}
\] \\

\subsection{Theorem} 
\bigskip

\textbf{Theorem :} Let E = $\mathbb{R}^n$, let A be a symmetric matrix in $\mathcal{M}_n(\mathbb{R})$. \\

Let us define $\phi(u,v) = {^T}X_1 A X_2$. \\

$\phi$ is a scalar product $\Leftrightarrow$ All eigenvalues of A are > 0.\\

Explanation in a an orthogonal eigenbasis $\mathcal{B'}$, $\phi(u,v) = {^TX'_1}DX'_2$ \\

Where D is a diagonal matrix.

\newpage 

\section{Orthogonal Projection}

Let $V_0$ be the orthogonal projection of $V$ onto $F$. \bigskip

$V_0$ is the element of $F$ the closest to $V$. \\

\subsection{Motivation}

Assume you must find $\lambda \in \mathbb{R}$ such that :

\[
v = \lambda u
\]

The "best solution" is $\lambda$ such that $v_0 = \lambda u$ \\

Ex: $u = (1,1), v = (2,1)$ \\

Solving $v = \lambda u : \begin{cases}
    \lambda = 2 \\
    \lambda = 1
\end{cases}$ \\

\[
v_0 = \alpha u = (\alpha, \alpha)
\]
\[(v - v_0) \perp u\]
\[(2 - \alpha, 1 - \alpha) \perp (1,1)\]
\[\implies (2 - \alpha) \times 1 + (1 - \alpha) \times 1 = 0\]
\[\implies 3 - 2\alpha = 0 \implies \alpha = \frac{3}{2}\]
\[\implies v_0 = \frac{3}{2}(1,1) = (\frac{3}{2}, \frac{3}{2})\]\\

The solution does not have a solution.\\

However, we can determine $v_0$.\\

$v_0$ represent the solution with the samellest range of error.

\subsection{Definition}

Let $(E, <, >)$ be an euclidean space. \\

Let $F$ be a linear subspace of E.

\[
F^\perp = \{ u \in E, \forall \, v \in F, <u,v> = 0 \}
\]

\subsection{Theorem}

If $F$ admits a basis $\mathcal{B} = (e_1, e_2, ..., e_n)$, then $F^\perp = \mathcal{B}^\perp$

\[
F^\perp = \{ u \in E, \begin{cases}
    <u, e_1> = 0 \\
    <u, e_2> = 0 \\
    ... \\
    <u, e_n> = 0
\end{cases}\}
\]

\subsection{Theorem}

If $F$ is finite-dimensional, then $F \oplus F^\perp = E$

\[
\Leftrightarrow \forall \, u \in E, \exists! (v,w) \in F \times F^\perp, u = v + w
\] \\

Then $v$ is the orthogonal projection of $u$ onto $F$.

\subsection{Property}

Let $u \in E$ and let $v_0 = P_F(u)$. \\

$P_F$ is the orthogonal projection onto $F$. \\

$v_0$ is the element of $F$ which minimizes $||u - v||^2$.\\

It is the solution of the optimization : \\

\[
Min(||u - v||), \text{ constraint : } v \in F
\]

\subsection{Proof}

Let $u = v_0 + w$ where $(v_0,w) \in F \times F^\perp$. \\

Then $\forall \, v \in F$ :

\[
||u - v||^2 = <u-v, u-v> \implies u - v = (u-v_0) + (v_0 - v)
\] \\

Indeed, $(u-v_0) = w \in F^\perp$ and $(v_0 - v) \in F$ as $(v_0, v) \in F^2$

\[
\implies ||u - v||^2 - <u-v, u-v> = <(u-v_0) + (v_0 - v), (u-v_0) + (v_0 - v)> 
\]

\[
= \,<(u - v_0), (u - v_0)> + \; 2 <(u - v_0), (v_0 - v)>  + <(v_0 - v), (v_0 - v)> 
\]

\[
= ||u - v_0||^2 + ||v_0 - v||^2 \geq ||u - v_0||^2
\]

\[
P_F(u) \in F \Leftrightarrow \exists (\lambda_1, ..., \lambda_n) \in \mathbb{R}^n, u = \lambda_1e_1 + ... + \lambda_ne_n
\]

\[
u - P_F(u) \in F^\perp \Leftrightarrow \begin{cases}
    <(u - \lambda_1e_1 + ... + \lambda_ne_n), e_1> = 0 \\
    ... \\
    <(u - \lambda_1e_1 + ... + \lambda_ne_n), e_n> = 0
\end{cases}
\] \\

\[
\Leftrightarrow \begin{cases}
    <(\lambda_1<e_1,e_1> + ... + \lambda_n<e_n,e_n>), e_1> = <u,e_1> \\
    ... \\
    <(\lambda_1<e_1,e_1> + ... + \lambda_n<e_n,e_n>), e_n> = <u, e_n>
\end{cases}
\] \\

\subsection{Finding $P_F(u)$}

Give $F$ admits a basis $\mathcal{B_F} = (\epsilon_1, ..., \epsilon_n)$ \\

We write $P_F(u) \in F \implies u - P_F(u) \in F^\perp$

\[
\implies u = P_F(u) + (u - P_F(u))
\]

\[
P_F(u) \in F \Leftrightarrow \exists (\alpha_1, ..., \alpha_n) \in \mathbb{R}^n, P_F(u) = \alpha_1\epsilon_1 + ... +  \alpha_n\epsilon_n
\]

\[
u - P_F(u) \in F^\perp \Leftrightarrow \begin{cases}
    <u - P_F(u), \epsilon_1> = 0 \\
    ... \\
    <u - P_F(u), \epsilon_n> = 0
\end{cases}
\] \\

\[
\Leftrightarrow \begin{cases}
    <u - \alpha_1\epsilon_1 + ... + \alpha_n\epsilon_n, \epsilon_1> = 0 \\
    ... \\
    <u - \alpha_1\epsilon_1 + ... + \alpha_n\epsilon_n, \epsilon_n> = 0
\end{cases}
\] \\

\[
\Leftrightarrow \begin{cases}
    <\alpha_1 <\epsilon_1, \epsilon_1> + ... + \alpha_n <\epsilon_n, \epsilon_n>, \epsilon_1> = <u, \epsilon_1> \\
    ... \\
    <\alpha_1 <\epsilon_1, \epsilon_1> + ... + \alpha_n <\epsilon_n, \epsilon_n>, \epsilon_n> = <u, \epsilon_n>
\end{cases}
\] \\


\end{document}